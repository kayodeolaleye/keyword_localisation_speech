{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "449bad60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import pickle\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "from config import trained_model_dir, device, pickle_file\n",
    "from os import path\n",
    "from models.attention import DotProductAttention\n",
    "from utils import parse_args, extract_feature_train\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad7d19e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_ids_lst = [\"1650981494_cnnattend_soft_random_data\", \"1651066062_cnnattend_soft_random_algo\", \n",
    "                  \"1651066062_cnnattend_soft_random_algo\", \"1651066000_cnnattend_soft_random_algo\",\n",
    "                  \"1651066000_cnnattend_soft_random_algo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f250049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(feature):\n",
    "    max_input_len = 800\n",
    "    input_length = feature.shape[0]\n",
    "    input_dim = feature.shape[1]\n",
    "    padded_input = np.zeros((max_input_len, input_dim), dtype=np.float32)\n",
    "    length = min(input_length, max_input_len)\n",
    "    padded_input[:length, :] = feature[:length, :]\n",
    "\n",
    "    padded_input = np.transpose(padded_input, (1, 0))\n",
    "\n",
    "    return padded_input, input_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11166f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parse_args\n",
    "with open(pickle_file, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "VOCAB = data[\"VOCAB_soft\"]\n",
    "samples = data[\"dev\"]\n",
    "num_samples = len(samples)\n",
    "\n",
    "sample = samples[0] \n",
    "wave = sample[\"wave\"]\n",
    "key = os.path.basename(wave).split(\".\")[0]\n",
    "# print(key)\n",
    "gt_trn = [i for i in sample[\"trn\"] if i in VOCAB]\n",
    "# target_dur = [(start_end, dur, tok) for (start_end, dur, tok) in sample[\"dur\"] if  tok.casefold() in VOCAB]\n",
    "feature = extract_feature_train(input_file=wave, feature='mfcc', dim=13, cmvn=True, delta=True, delta_delta=True)\n",
    "# feature = (feature - feature.mean()) / feature.std()\n",
    "padded_input, input_length = pad(feature)\n",
    "padded_input = torch.from_numpy(padded_input).unsqueeze(0).to(device)\n",
    "input_length = torch.tensor([input_length]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a5ed67a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 39, 800])\n",
      "tensor([801], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(padded_input.shape)\n",
    "print(input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ced769c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_models(models_ids_lst):\n",
    "    all_models = []\n",
    "    all_conv_feat_model = []\n",
    "    for model_id in models_ids_lst:\n",
    "        checkpoint_path = path.join(trained_model_dir, model_id, \"BEST_checkpoint.tar\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "        model = checkpoint[\"model\"].to(device)\n",
    "        all_conv_feat_model.append(nn.Sequential(*list(model.children()))[0])\n",
    "        all_models.append(model)       \n",
    "    return all_models, all_conv_feat_model\n",
    "\n",
    "def make_trainable_false(all_models, all_conv_feat):\n",
    "    for model in all_models:\n",
    "        model.eval()\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "    for model in all_conv_feat:\n",
    "        model.eval()\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "    return all_models, all_conv_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae999093",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kayode/miniconda3/envs/phd3.0/lib/python3.7/site-packages/torch/serialization.py:657: SourceChangeWarning: source code of class 'models.cnnattend.CNNAttend' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/kayode/miniconda3/envs/phd3.0/lib/python3.7/site-packages/torch/serialization.py:657: SourceChangeWarning: source code of class 'models.attention.DotProductAttention' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    }
   ],
   "source": [
    "all_models_trainable, all_conv_feat_model_trainable = load_all_models(models_ids_lst)\n",
    "all_models_not_trainable, all_conv_feat_model_not_trainable = make_trainable_false(all_models_trainable, all_conv_feat_model_trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4b6b099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000, 800])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_conv_feat_model_not_trainable[0](padded_input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c6157ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([67])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_models_not_trainable[0](padded_input)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93fa3bb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 67, 800])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_models_not_trainable[0](padded_input)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c126a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = MyEnsemble(all_models_not_trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ef020999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class EnsembleCNNAttend(nn.Module):\n",
    "\n",
    "#     def __init__(self, vocab_size, embed_size, all_models, all_conv_feat_models):\n",
    "#         super(EnsembleCNNAttend, self).__init__()\n",
    "\n",
    "#         # Ensemble inputs\n",
    "#         self.input_layers = get_multi_headed_input(all_models)\n",
    "#         # Full model\n",
    "#         self.all_models = all_models\n",
    "#         # Convolutional module\n",
    "#         self.all_conv_feat_models = all_conv_feat_models\n",
    "\n",
    "#         # Embedding module\n",
    "#         self.embed = embed_queries(embed_size, vocab_size)\n",
    "        \n",
    "#         # Attention module\n",
    "#         self.attention_module = DotProductAttention()\n",
    "        \n",
    "        \n",
    "        \n",
    "#         # MLP module\n",
    "#         self.mlp_module = nn.Sequential(\n",
    "#             nn.Linear(1000, 512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(512, 1),\n",
    "#         )\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         print('x: ', x.shape)\n",
    "#         ensemble_inputs = [input_layer(x) for input_layer in self.input_layers]\n",
    "#         ensemble_outputs = [model(x) for model in self.all_models]\n",
    "#         ensemble_conv_feats = [conv_feat_model(x) for conv_feat_model in self.all_conv_feat_models]\n",
    "#         detect_out = [out[0] for out in ensemble_outputs]\n",
    "#         out = torch.cat(detect_out) #5x67\n",
    "#         conv_feats = torch.cat(ensemble_conv_feats) #5x1000x800\n",
    "#         print(\"Conv_feat shape: \", conv_feats.shape)\n",
    "#         print('embed shape: ', self.embed.shape)\n",
    "        \n",
    "#         context_vector, attention_weights = self.attention_module(self.embed.cuda(), conv_feats)\n",
    "#         print('context_vector shape', context_vector.shape)\n",
    "# # #         context_vector = torch.flatten(context_vector)\n",
    "# #         context_vector = self.dense_1(context_vector)\n",
    "# #         context_vector = F.relu(context_vector)\n",
    "# #         output = self.dense_2(context_vector)\n",
    "#         output = self.mlp_module(context_vector)\n",
    "#         output = torch.mean(output, dim = 0).squeeze()\n",
    "#         attention_weights = torch.mean(attention_weights, dim = 0)\n",
    "#         print(\"Output: \", output.shape)\n",
    "#         print(\"attention_weights: \", attention_weights.shape)\n",
    "#         return output, attention_weights\n",
    "\n",
    "# def embed_queries(embed_size, vocab_size):\n",
    "\n",
    "#     q_embed = torch.zeros(vocab_size, embed_size)\n",
    "#     embeddings = nn.Embedding(vocab_size, embed_size)\n",
    "#     for i in range(vocab_size):\n",
    "#         lookup_tensor = torch.tensor([i], dtype=torch.long)\n",
    "#         embed = embeddings(lookup_tensor)\n",
    "        \n",
    "#         q_embed[i, :] = embed\n",
    "        \n",
    "#     return q_embed\n",
    "\n",
    "# def get_multi_headed_input(all_models):\n",
    "    \n",
    "#     ensemble_models = [nn.Sequential(*list(model.children())[0]) for model in all_models]\n",
    "#     ensemble_inputs = [ensemble_input[0] for ensemble_input in ensemble_models]\n",
    "    \n",
    "#     return ensemble_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a6003393",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleCNNAttend(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, all_models, all_conv_feat_models):\n",
    "        super(EnsembleCNNAttend, self).__init__()\n",
    "\n",
    "        # Ensemble inputs\n",
    "        self.input_layers = get_multi_headed_input(all_models)\n",
    "        # Full model\n",
    "        self.all_models = all_models\n",
    "        # Convolutional module\n",
    "        self.all_conv_feat_models = all_conv_feat_models\n",
    "\n",
    "        # Embedding module\n",
    "        self.embed = embed_queries(embed_size, vocab_size)\n",
    "        \n",
    "        # Attention module\n",
    "        self.attention_module = DotProductAttention()\n",
    "        \n",
    "        \n",
    "        \n",
    "        # MLP module\n",
    "        self.mlp_module = nn.Sequential(\n",
    "            nn.Linear(335, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 67),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        print('x: ', x.shape)\n",
    "        ensemble_inputs = [input_layer(x) for input_layer in self.input_layers]\n",
    "        ensemble_outputs = [model(x) for model in self.all_models]\n",
    "        ensemble_conv_feats = [conv_feat_model(x) for conv_feat_model in self.all_conv_feat_models]\n",
    "        detect_out = [out[0] for out in ensemble_outputs]\n",
    "        out = torch.cat(detect_out) #5x67\n",
    "        conv_feats = torch.cat(ensemble_conv_feats) #5x1000x800\n",
    "        print(\"Conv_feat shape: \", conv_feats.shape)\n",
    "        print('embed shape: ', self.embed.shape)\n",
    "        \n",
    "        context_vector, attention_weights = self.attention_module(self.embed.cuda(), conv_feats)\n",
    "        print('context_vector shape', context_vector.shape)\n",
    "\n",
    "        output = self.mlp_module(out)\n",
    "#         output = torch.mean(output, dim = 0).squeeze()\n",
    "        attention_weights = torch.mean(attention_weights, dim = 0)\n",
    "        print(\"Output: \", output.shape)\n",
    "        print(\"attention_weights: \", attention_weights.shape)\n",
    "        return output, attention_weights\n",
    "\n",
    "def embed_queries(embed_size, vocab_size):\n",
    "\n",
    "    q_embed = torch.zeros(vocab_size, embed_size)\n",
    "    embeddings = nn.Embedding(vocab_size, embed_size)\n",
    "    for i in range(vocab_size):\n",
    "        lookup_tensor = torch.tensor([i], dtype=torch.long)\n",
    "        embed = embeddings(lookup_tensor)\n",
    "        \n",
    "        q_embed[i, :] = embed\n",
    "        \n",
    "    return q_embed\n",
    "\n",
    "def get_multi_headed_input(all_models):\n",
    "    \n",
    "    ensemble_models = [nn.Sequential(*list(model.children())[0]) for model in all_models]\n",
    "    ensemble_inputs = [ensemble_input[0] for ensemble_input in ensemble_models]\n",
    "    \n",
    "    return ensemble_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7f30e0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EnsembleCNNAttend(67, 1000, all_models_not_trainable, all_conv_feat_model_not_trainable).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "47afc658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  torch.Size([1, 39, 800])\n",
      "Conv_feat shape:  torch.Size([5, 1000, 800])\n",
      "embed shape:  torch.Size([67, 1000])\n",
      "context_vector shape torch.Size([5, 67, 1000])\n",
      "Output:  torch.Size([67])\n",
      "attention_weights:  torch.Size([67, 800])\n"
     ]
    }
   ],
   "source": [
    "out, attention_weight = model(padded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f5fa0efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3983, 0.5787, 0.3388, 0.6601, 0.5761, 0.4613, 0.6132, 0.1628, 0.6192,\n",
       "        0.7172, 0.2977, 0.2564, 0.0983, 0.4015, 0.7439, 0.6216, 0.4193, 0.2686,\n",
       "        0.3893, 0.5928, 0.1900, 0.2723, 0.7151, 0.4519, 0.7079, 0.3063, 0.6007,\n",
       "        0.4782, 0.5449, 0.8484, 0.7223, 0.1655, 0.6226, 0.5478, 0.3525, 0.4244,\n",
       "        0.5485, 0.7773, 0.3444, 0.4673, 0.5368, 0.1594, 0.2824, 0.5200, 0.3643,\n",
       "        0.3572, 0.5015, 0.6881, 0.5866, 0.6449, 0.4972, 0.7838, 0.2931, 0.1076,\n",
       "        0.5564, 0.7423, 0.5796, 0.1802, 0.4626, 0.5411, 0.1661, 0.6574, 0.3553,\n",
       "        0.6071, 0.1497, 0.2439, 0.5995], device='cuda:0',\n",
       "       grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.sigmoid(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5d779a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(attention_weight.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "4780d069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1722, -0.1963, -0.2076,  ..., -0.0741, -0.0669, -0.0509],\n",
       "        [ 0.0045, -0.0409, -0.0283,  ..., -0.0427, -0.0281, -0.0163],\n",
       "        [-0.0676, -0.0577, -0.0653,  ...,  0.0078,  0.0222,  0.0196],\n",
       "        ...,\n",
       "        [-0.0162, -0.0669, -0.0678,  ..., -0.0022, -0.0048, -0.0231],\n",
       "        [ 0.0682,  0.0870,  0.0719,  ...,  0.0424,  0.0407,  0.0604],\n",
       "        [ 0.0767, -0.0054, -0.0587,  ..., -0.0135,  0.0382,  0.0683]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([x(padded_input) for x in all_conv_feat_model_not_trainable])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "95607961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv1d(39, 96, kernel_size=(9,), stride=(1,), padding=(4,))\n",
       "  (1): LeakyReLU(negative_slope=0.01)\n",
       "  (2): Conv1d(96, 96, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "  (3): ReLU()\n",
       "  (4): Conv1d(96, 96, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "  (5): LeakyReLU(negative_slope=0.01)\n",
       "  (6): Conv1d(96, 96, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "  (7): LeakyReLU(negative_slope=0.01)\n",
       "  (8): Conv1d(96, 96, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "  (9): LeakyReLU(negative_slope=0.01)\n",
       "  (10): Conv1d(96, 1000, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       ")"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_conv_feat_model_not_trainable[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "2fba9db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 67, 800])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(torch.rand(67, 1000), torch.rand(5, 1000, 800)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e667c9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define multi headed input\n",
    "\n",
    "def get_multi_headed_input(all_models):\n",
    "    ensemble_models = [nn.Sequential(*list(model.children())[0]) for model in all_models]\n",
    "    ensemble_inputs = [ensemble_input[0] for ensemble_input in ensemble_models]\n",
    "    return ensemble_inputs\n",
    "\n",
    "def get_model_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e978335e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_inputs = get_multi_headed_input(all_models_not_trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "cc2b58df",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (data) in enumerate(train_loader):\n",
    "    # Move to GPU, if available\n",
    "    optimizer.zero_grad()\n",
    "    target = None\n",
    "    padded_input, bow_target, soft_target, _, input_lengths = data\n",
    "    # print(\"padded input: \", padded_input[0][0][:2])\n",
    "    padded_input = padded_input.to(device)\n",
    "    input_lengths = input_lengths.to(device)\n",
    "    if target_type == 'bow':\n",
    "        target = bow_target.to(device)\n",
    "        continue\n",
    "    elif target_type == 'soft':\n",
    "        target = soft_target.to(device)\n",
    "    else:\n",
    "        print(\"Incorrect supervision's target. Choose either 'bow' or 'soft'.\")\n",
    "        break\n",
    "\n",
    "    # Forward prop.\n",
    "    out, attention_Weights = model(padded_input)\n",
    "    # loss = criterion(torch.sigmoid(out), target)\n",
    "    # print(\"Out Shape: \", out.shape)\n",
    "    # print(\"target shape: \", target.shape)\n",
    "    loss = criterion(out, target)\n",
    "\n",
    "    # Back prop.\n",
    "    loss.backward(retain_graph=True)\n",
    "\n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # Keep track of metrics\n",
    "    losses.update(loss.item())\n",
    "\n",
    "    # Print status\n",
    "    if i % print_freq == 0:\n",
    "        logger.info('Epoch: [{0}][{1}/{2}]\\t'\n",
    "        'Loss {loss.val:.4f} ({loss.avg:.4f})'.format(epoch, i, len(train_loader), loss=losses))\n",
    "\n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "    def valid(valid_loader, model, logger, threshold):\n",
    "    model.eval()\n",
    "    losses = AverageMeter()\n",
    "    n_tp = 0  \n",
    "    n_tp_fp = 0 # (tp + fp)\n",
    "    n_tp_fn = 0 # (tp + fn)\n",
    "\n",
    "    # Create loss function\n",
    "    # criterion = nn.BCELoss()\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Batches\n",
    "    for data in tqdm(valid_loader):\n",
    "    # Move to GPU, if available\n",
    "    padded_input, bow_target, _, __, input_lengths = data\n",
    "    # padded_input = torch.transpose(padded_input, 2, 1)\n",
    "    padded_input = padded_input.to(device)\n",
    "    input_lengths = input_lengths.to(device)\n",
    "    target = bow_target.to(device)\n",
    "    # Forward prop.\n",
    "    out, attention_weights = model(padded_input)\n",
    "    # loss = criterion(torch.sigmoid(out), target)\n",
    "    loss = criterion(out, target)\n",
    "\n",
    "    # Keep track of metrics\n",
    "    losses.update(loss.item())\n",
    "    sigmoid_out = torch.sigmoid(out).cpu()\n",
    "    # sigmoid_out = out.cpu()\n",
    "    sigmoid_out_thresholded = torch.ge(sigmoid_out, threshold).float()\n",
    "    n_tp += torch.sum(sigmoid_out_thresholded * target.cpu()).numpy()\n",
    "    n_tp_fp += torch.sum(sigmoid_out_thresholded).numpy()\n",
    "    n_tp_fn += torch.sum(target.cpu()).numpy()\n",
    "\n",
    "    precision = n_tp / n_tp_fp\n",
    "    recall = n_tp / n_tp_fn\n",
    "    fscore = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    # Print status\n",
    "    logger.info('\\nValidation Loss {loss.val:.4f} ({loss.avg:.4f})\\n'.format(loss=losses))\n",
    "    logger.info('\\nValidation Precision: {precision:.4f}\\n'.format(precision=precision))\n",
    "    logger.info('\\nValidation Recall: {recall:.4f}\\n'.format(recall=recall))\n",
    "    logger.info('\\nValidation F-score: {fscore:.4f}\\n'.format(fscore=fscore))\n",
    "\n",
    "    return losses.avg, precision, recall, fscore"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
